(window.webpackJsonp=window.webpackJsonp||[]).push([[50],{438:function(r,t,a){"use strict";a.r(t);var e=a(21),i=Object(e.a)({},(function(){var r=this,t=r.$createElement,a=r._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[a("h1",{attrs:{id:"第一章-分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#第一章-分类"}},[r._v("#")]),r._v(" 第一章-分类")]),r._v(" "),a("p",[a("font",{attrs:{color:"red",size:"6"}},[r._v("万丈高楼平地起")])],1),r._v(" "),a("p",[r._v("并行计算的分类从两个角度区分，同时介绍并行计算的编程模型。")]),r._v(" "),a("h2",{attrs:{id:"费林分类法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#费林分类法"}},[r._v("#")]),r._v(" 费林分类法")]),r._v(" "),a("p",[r._v("Flynn's classic taxonomy，从指令和数据角度看待。用一幅图描述费林分类法：")]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/flynn.png",width:"600",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"sisd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sisd"}},[r._v("#")]),r._v(" SISD")]),r._v(" "),a("p",[r._v("Single Instruction, Single Data.单指令单数据流。")]),r._v(" "),a("ul",[a("li",[r._v("串行计算机")]),r._v(" "),a("li",[r._v("单指令：在处理器上一个时钟周期只有一个指令流被执行")]),r._v(" "),a("li",[r._v("单数据：在一个时钟周期只有一个数据流被使用")]),r._v(" "),a("li",[r._v("例子：single-core processor")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/sisd.jpg",width:"500",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"simd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#simd"}},[r._v("#")]),r._v(" SIMD")]),r._v(" "),a("p",[r._v("Single Instruction, Multiple Data.单指令多数据流。")]),r._v(" "),a("ul",[a("li",[r._v("单指令：所有的处理单元处理相同的指令")]),r._v(" "),a("li",[r._v("多数据：不同的处理单元处理不同的数据")]),r._v(" "),a("li",[r._v("例子：GPU，vector processor (X86 AVX instruction)")])]),r._v(" "),a("p",[r._v("在cuda编程中是非常典型的应用。不同的thread使用相同的代码，通过不同的数据索引来处理不同位置的数据。")]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/simd.jpg",width:"600",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"misd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#misd"}},[r._v("#")]),r._v(" MISD")]),r._v(" "),a("p",[r._v("Multiple Instruction, Single Data.多数据单指令流。")]),r._v(" "),a("ul",[a("li",[r._v("多指令：每个处理器执行不同的指令流")]),r._v(" "),a("li",[r._v("单数据：一个数据流被输入进不同的处理单元")]),r._v(" "),a("li",[r._v("例子：只有1971年CMU实现，能被用于 fault tolerance")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/misd.jpg",width:"600",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"mimd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mimd"}},[r._v("#")]),r._v(" MIMD")]),r._v(" "),a("p",[r._v("Multiple Instruction, Multiple Data.多指令多数据流。")]),r._v(" "),a("ul",[a("li",[r._v("多指令：每个处理器处理不同的指令流")]),r._v(" "),a("li",[r._v("多数据：每个处理器处理不同的数据流")]),r._v(" "),a("li",[r._v("例子：Most modern computers，如multi-core CPU")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/mimd.jpg",width:"600",alt:"串行"}})]),r._v(" "),a("h2",{attrs:{id:"内存架构分类法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#内存架构分类法"}},[r._v("#")]),r._v(" 内存架构分类法")]),r._v(" "),a("p",[r._v("Memory architecture classification。从内存架构的角度来对并行计算分类。Shared Memory Computer Architecture、Distributed Memory Computer Architecture 和 Hybrid Distributed-Shared Memory。")]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/memory_arch.png",width:"700",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"shared-memory-computer-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-computer-architecture"}},[r._v("#")]),r._v(" Shared Memory Computer Architecture")]),r._v(" "),a("p",[r._v("共享内存的并行计算架构有分为：Non-Uniform Memory Access 和 Non-Uniform Memory Access。")]),r._v(" "),a("h4",{attrs:{id:"uniform-memory-access-uma"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#uniform-memory-access-uma"}},[r._v("#")]),r._v(" Uniform Memory Access (UMA)")]),r._v(" "),a("ul",[a("li",[r._v("在今天大多是对称多处理器系统 SMP (Symmetric Multiprocessor machines)")]),r._v(" "),a("li",[r._v("处理器相同")]),r._v(" "),a("li",[r._v("访存 memory 时间相同")]),r._v(" "),a("li",[r._v("例子：商业服务器")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/UMA.png",width:"450",alt:"串行"}})]),r._v(" "),a("h4",{attrs:{id:"non-uniform-memory-access-numa"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#non-uniform-memory-access-numa"}},[r._v("#")]),r._v(" Non-Uniform Memory Access (NUMA)")]),r._v(" "),a("ul",[a("li",[r._v("通常由至少两个SMP通过物理"),a("strong",[r._v("连接")]),r._v("组成")]),r._v(" "),a("li",[r._v("一个 SMP 能够直接访存另一个 SMP 的 Memory")]),r._v(" "),a("li",[r._v("不同 SMP 之间通过"),a("strong",[r._v("连接")]),r._v("访存 memory 的速度"),a("strong",[r._v("比较慢")])]),r._v(" "),a("li",[r._v("例子：HPC Server")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/NUMA.png",width:"600",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"distributed-memory-computer-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#distributed-memory-computer-architecture"}},[r._v("#")]),r._v(" Distributed Memory Computer Architecture")]),r._v(" "),a("ul",[a("li",[r._v("需要"),a("strong",[r._v("连接网络")])]),r._v(" "),a("li",[r._v("处理器有自己的"),a("strong",[r._v("内存和地址空间")])]),r._v(" "),a("li",[r._v("一个处理器内存的变化不影响其他处理器的内存")]),r._v(" "),a("li",[r._v("程序员或者编程工具需要"),a("strong",[r._v("显示")]),r._v("的定义：how and when data is communicated between processors")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/DisMemArch.png",width:"600",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"hybrid-distributed-shared-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hybrid-distributed-shared-memory"}},[r._v("#")]),r._v(" Hybrid Distributed-Shared Memory")]),r._v(" "),a("ul",[a("li",[r._v("当今的计算机既有共享内存架构也有分布式内存架构")]),r._v(" "),a("li",[r._v("共享内存 component 可以是共享内存机器")]),r._v(" "),a("li",[r._v("分布式内存 component 由网络连接的多台共享内存机器组成")]),r._v(" "),a("li",[r._v("未来的并行计算架构会一直是混合模式")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/hybrid_mem.png",width:"600",alt:"串行"}})]),r._v(" "),a("h2",{attrs:{id:"并行编程模型分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#并行编程模型分类"}},[r._v("#")]),r._v(" 并行编程模型分类")]),r._v(" "),a("p",[r._v("Parallel Programming model classification。并行计算编程模型是在硬件和内存架构的基础之上"),a("strong",[r._v("抽象")]),r._v("出来的结构，分类为：Shared Memory Programming Model、Message Passing Programming Model 和 Hybrid Programming Model。")]),r._v(" "),a("p",[r._v("通常的编程模型是为了匹配计算机的物理架构，")]),r._v(" "),a("p",[r._v("◇ 共享内存的编程模型匹配共享内存的计算机架构 "),a("br"),r._v("\n◇ 消息传递的编程模型匹配分布式内存计算机架构 "),a("br")]),r._v(" "),a("p",[r._v("但是编程模型并不需要严格的受限于计算机架构，")]),r._v(" "),a("p",[r._v("◇ 消息传递编程模型可用在共享内存的计算机架构上。例：把 MPI 用到单台服务器上 "),a("br"),r._v("\n◇ 共享内存编程模型可用在分布式内存计算机架构上。例：Partitioned Global Address Space "),a("br")]),r._v(" "),a("h3",{attrs:{id:"shared-memory-programming-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-programming-model"}},[r._v("#")]),r._v(" Shared Memory Programming Model")]),r._v(" "),a("ul",[a("li",[r._v("一个处理器拥有多个、并发的处理路径")]),r._v(" "),a("li",[r._v("Threads 有局部数据，但是同时有共享的资源")]),r._v(" "),a("li",[r._v("Threads 之间通过 global memory 沟通")]),r._v(" "),a("li",[r._v("Threads 能够创建和销毁，但是主程序要保留：提供必要的共享资源直到程序完成")])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/threads.JPG",width:"800",alt:"串行"}})]),r._v(" "),a("p",[r._v("共享内存并行计算编程模型实现方法：")]),r._v(" "),a("ul",[a("li",[r._v("A library of subroutines called from parallel source code, E.g.: POSIX Thread (Pthread)")]),r._v(" "),a("li",[r._v("A set of compiler directives imbedded in either serial or parallel source code. E.g.: OpenMP")])]),r._v(" "),a("h3",{attrs:{id:"message-passing-programming-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#message-passing-programming-model"}},[r._v("#")]),r._v(" Message Passing Programming Model")]),r._v(" "),a("ul",[a("li",[r._v("每个 task 在处理任务时使用自己的局部内存，多个 tasks 能够经过任意数量的物理机器访问同一个机器")]),r._v(" "),a("li",[r._v("多个 tasks 的数据交换通过：communications by sending and receiving messages")]),r._v(" "),a("li",[r._v("实现：MPI")])]),r._v(" "),a("p",[a("strong",[r._v("共享内存并行编程模型")]),r._v(" 和 "),a("strong",[r._v("消息传递并行编程模型")]),r._v(" 的对比：")]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/shared_dis.png",width:"700",alt:"串行"}})]),r._v(" "),a("h3",{attrs:{id:"hybrid-programming-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hybrid-programming-model"}},[r._v("#")]),r._v(" Hybrid Programming Model")]),r._v(" "),a("ul",[a("li",[a("p",[r._v("由上述几种模型混合组成")])]),r._v(" "),a("li",[a("p",[r._v("混合模型适用于现有的硬件和软件架构")])]),r._v(" "),a("li",[a("p",[r._v("例子：message passing model (MPI) 和 threads model (OpenMP)的混合，如图：\n")]),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/hybrid_model.jpg",width:"500",alt:"串行"}})]),a("p")]),r._v(" "),a("li",[a("p",[r._v("例子：message passing model (MPI) 和 CPU-GPU 混合")]),r._v(" "),a("ul",[a("li",[r._v("MPI 程序跑在 CPUs 上，使用局部内存，通过网路进行通信")]),r._v(" "),a("li",[r._v("密集计算的程序跑在 GPUs 上")]),r._v(" "),a("li",[r._v("数据在单机内存和 GPU 之间的传输通过 CUDA 实现")])])])]),r._v(" "),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/hybrid_model2.jpg",width:"500",alt:"串行"}})]),r._v(" "),a("h2",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[r._v("#")]),r._v(" 总结")]),r._v(" "),a("p",[r._v("⭐️ 编程模型和并行系统架构的发展是相互影响的")]),r._v(" "),a("p",[r._v("⭐️ openMP, MPI, Pthreads, CUDA 只是做并行计算的编程语言")]),r._v(" "),a("p",[r._v("⭐️ 知道什么是并行计算比怎么做并行计算更加重要：")]),r._v(" "),a("p",[r._v("   ♦ 更快的学习新的并行编程工具 "),a("br"),r._v("\n   ♦ 理解自己程序的性能 "),a("br"),r._v("\n   ♦ 优化自己程序的性能 "),a("br")]),r._v(" "),a("hr"),r._v(" "),a("p",[r._v("最后用一幅图来说明并行计算的发展趋势：\n")]),a("div",{attrs:{align:"center"}},[a("img",{attrs:{src:"/images/parallel_computing/1/trend.JPG",alt:"串行"}})]),a("p"),r._v(" "),a("h2",{attrs:{id:"参考资料"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[r._v("#")]),r._v(" 参考资料")]),r._v(" "),a("p",[a("a",{attrs:{href:"https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial",target:"_blank",rel:"noopener noreferrer"}},[r._v("Introduction parallel computing tutorial"),a("OutboundLink")],1)]),r._v(" "),a("p",[r._v("台湾新竹清华大学 "),a("a",{attrs:{href:"https://ocw.nthu.edu.tw/ocw/index.php?page=course&cid=231",target:"_blank",rel:"noopener noreferrer"}},[r._v("平行程式"),a("OutboundLink")],1)])])}),[],!1,null,null,null);t.default=i.exports}}]);